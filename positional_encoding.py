import math

import torch
import torch.nn as nn
from einops import rearrange, repeat


def generate_causal_mask(batch_size, num_heads, seq_len):
    # 创建基础掩码（下三角矩阵）
    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    # 扩展掩码到批次+ num_heads维度
    mask = mask.unsqueeze(0).expand(batch_size, num_heads, -1, -1)
    return mask


def get_slopes(n_heads):
    def get_slopes_power_of_2(n_heads):
        start = (2 ** (-2 ** -(math.log2(n_heads) - 3)))
        ratio = start
        return [start * ratio ** i for i in range(n_heads)]

    if math.log2(n_heads).is_integer():
        return get_slopes_power_of_2(n_heads)
    else:
        closest_power_of_2 = 2 ** math.floor(math.log2(n_heads))
        return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2 * closest_power_of_2)[0::2][
                                                           :n_heads - closest_power_of_2]


def generate_positions(seq_length, device):
    return torch.arange(seq_length, dtype=torch.float32, device=device).unsqueeze(0)


def generate_rev_distance_matrix(seq_length, device):
    distances = generate_positions(seq_length, device)
    return distances.unsqueeze(1) - distances.unsqueeze(2)


def build_alibi_tensor(attention_mask: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:
    batch_size, num_heads, seq_length, _ = attention_mask.shape
    # 生成距离矩阵
    distances = generate_rev_distance_matrix(seq_length, device=attention_mask.device)
    # 获取每个头的斜率
    slopes = torch.tensor(get_slopes(num_heads), dtype=dtype, device=attention_mask.device)
    # 计算alibi偏置
    alibi = slopes.view(1, num_heads, 1, 1) * distances.unsqueeze(1)
    return alibi


class CoPE(nn.Module):
    def __init__(self, npos_max, head_dim):
        super().__init__()
        self.npos_max = npos_max
        self.pos_emb = nn.parameter.Parameter(
            torch.zeros(1, head_dim, npos_max)
        )

    def forward(self, query, attn_logits):
        # compute positions
        gates = torch.sigmoid(attn_logits)
        pos = gates.flip(-1).cumsum(dim=-1).flip(-1)
        pos = pos.clamp(max=self.npos_max - 1)

        # interpolate from integer positions
        pos_ceil = pos.ceil().long()
        pos_floor = pos.floor().long()
        logits_int = torch.matmul(query, self.pos_emb)
        logits_ceil = logits_int.gather(-1, pos_ceil)
        logits_floor = logits_int.gather(-1, pos_floor)
        w = pos - pos_floor

        return logits_ceil * w + logits_floor * (1 - w)


class SelfAttn(nn.Module):
    def __init__(self, npos_max, head_dim):
        super().__init__()
        self.cope = CoPE(npos_max, head_dim)
        self.head_dim = head_dim

    def forward(self, query, key, val, mask):
        # q, k, v have dimensions batch x seq_len x head_dim
        attn_logits = torch.bmm(query, key.transpose(-1, -2))
        attn_logits = attn_logits / math.sqrt(self.head_dim)
        attn_logits += mask.log()
        attn_logits += self.cope(query, attn_logits)
        attn = torch.softmax(attn_logits, dim=-1)
        out = torch.bmm(attn, val)
        return out


class DAPE(nn.Module):
    def __init__(self, num_heads=12, mlp_width=32):
        """
        DAPE attention bias module.

        Args:
            num_heads: number of attention heads.
            mlp_width: Width of MLP.
        """
        super(DAPE, self).__init__()

        self.mlp = nn.Sequential(
            nn.Linear(2 * num_heads, mlp_width),
            nn.LeakyReLU(),
            nn.Linear(mlp_width, num_heads)
        )

    def forward(self, attention: torch.Tensor, bias: torch.Tensor):
        """
        Args:
            attention: input sequence, which is q^T * k,
                shape [bsz, num_heads, seq_len, seq_len]
            bias: bias matrix, which can be generated by Alibi, Kerple
                FIRE or other additive position encodings
                shape [1, num_heads, seq_len, seq_len]

        Returns:
            attention with DAPE,
            shape [bsz, num_heads, seq_len, seq_len]
        """
        # Expand bias to match the attention shape
        bias_tile = repeat(bias, '1 h T T -> b h T T', b=attention.shape[0])

        # Concatenate attention and bias along the head dimension
        attention_bias_concat = torch.cat((attention, bias_tile), dim=1)

        # Rearrange dimensions for MLP processing
        attention_bias_concat = rearrange(attention_bias_concat, 'b h T T -> b T T h')

        # Apply the MLP to the concatenated attention and bias
        attention_bias_concat = self.mlp(attention_bias_concat)

        # Rearrange back to original dimensions
        attention_bias_concat = rearrange(attention_bias_concat, 'b T T h -> b h T T')

        # Return the sum of the original attention and the bias processed through the MLP
        return attention + bias + attention_bias_concat


def test_build_alibi_tensor():
    # 测试用例1：基本功能测试
    batch_size = 1
    num_heads = 1
    seq_len = 9
    pos = 3

    input_ids = torch.ones(batch_size, seq_len)
    device = "cpu"

    print(1111, get_slopes(num_heads))

    print(2222, generate_positions(seq_len, device))

    print(3333, generate_rev_distance_matrix(seq_len, device))

    attn_mask = generate_causal_mask(batch_size, num_heads, seq_len)
    print(4444, build_alibi_tensor(attn_mask, dtype=torch.float32))


if __name__ == "__main__":
    # 运行测试
    test_build_alibi_tensor()
